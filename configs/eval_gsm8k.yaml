experiment:
  name: eval_gsm8k
  seed: 123
  output_dir: ${env:OUTPUT_DIR}/evals/${experiment.name}

model:
  # Base model to load (required for LoRA checkpoints)
  base_model: meta-llama/Meta-Llama-3.1-8B-Instruct
  # Path to LoRA adapter checkpoint
  adapter_path: ${env:OUTPUT_DIR}/runs/llama31_8b_forward_backward_dpo/checkpoint-200
  dtype: bf16
  load_in_8bit: true

inference:
  max_new_tokens: 1024
  temperature: 0.6
  top_p: 0.95
  do_sample: true
  forward_reasoning_prompt: prompts/forward_template.txt
  backward_reasoning_prompt: prompts/backward_template.txt
  num_backward_passes: 2
  majority_vote: true

metrics:
  - name: accuracy
    type: exact_match
  - name: self_consistency
    type: diversity
  - name: acknowledgement_rate
    type: backward_self_assess

datasets:
  - name: gsm8k
    split: test
    path: openai/gsm8k
    subset: main
    few_shot_k: 8
