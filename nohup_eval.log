00:21:52 | Logging to: /home/DAVIDSON/munikzad/csc224n/reasoning/experiment_outputs/logs/eval_batch_20260110_002152.log
Logging to: /home/DAVIDSON/munikzad/csc224n/reasoning/experiment_outputs/logs/eval_batch_20260110_002152.log
00:21:52 | ============================================================
00:21:52 | OPTIMIZED BATCH EVALUATION
00:21:52 | ============================================================
00:21:52 | Base model: meta-llama/Meta-Llama-3.1-8B-Instruct
00:21:52 | Output dir: /home/DAVIDSON/munikzad/csc224n/reasoning/experiment_outputs/evals
00:21:52 | Models: ['baseline', 'forward_only', 'backward_only', 'hybrid_60_40']
00:21:52 | Max samples: 350
00:21:52 | Resuming - already completed: ['baseline', 'forward_only']
00:21:52 | Loading GSM8K dataset...
00:21:55 | Dataset loaded: 1319 test samples
[00:22:04] Loading base model: meta-llama/Meta-Llama-3.1-8B-Instruct                           eval_all_models.py:123
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                              | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|█████████████▌                                        | 1/4 [00:11<00:35, 11.94s/it]Loading checkpoint shards:  50%|███████████████████████████                           | 2/4 [00:17<00:16,  8.05s/it]Loading checkpoint shards:  75%|████████████████████████████████████████▌             | 3/4 [00:25<00:08,  8.23s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 4/4 [00:26<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 4/4 [00:26<00:00,  6.55s/it]
[00:22:32] Base model loaded successfully                                                      eval_all_models.py:177
00:22:32 | Base model loaded in 37.2s
00:22:32 | Skipping baseline (already completed)
00:22:32 | Skipping forward_only (already completed)
00:22:32 | ============================================================
00:22:32 | Evaluating: backward_only
00:22:32 | ============================================================
00:22:32 | Loading adapter: /home/DAVIDSON/munikzad/csc224n/reasoning/experiment_outputs/runs/backward_only_dpo
00:22:32 |   ✓ Adapter loaded: backward_only_dpo
00:22:32 | Resuming from sample 150/350
00:22:32 | Evaluating backward_only: 350 samples
The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
/home/DAVIDSON/munikzad/.conda/envs/reasoning/lib/python3.13/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
00:37:55 |   [160/350] Accuracy so far: 135/160 = 84.4%
00:52:42 |   [170/350] Accuracy so far: 143/170 = 84.1%
01:06:13 |   [180/350] Accuracy so far: 153/180 = 85.0%
01:21:01 |   [190/350] Accuracy so far: 159/190 = 83.7%
01:34:57 |   [200/350] Accuracy so far: 168/200 = 84.0%
01:34:57 |   Checkpoint saved at sample 200
01:49:07 |   [210/350] Accuracy so far: 176/210 = 83.8%
02:02:58 |   [220/350] Accuracy so far: 184/220 = 83.6%
02:16:22 |   [230/350] Accuracy so far: 193/230 = 83.9%
02:31:02 |   [240/350] Accuracy so far: 202/240 = 84.2%
02:45:39 |   [250/350] Accuracy so far: 209/250 = 83.6%
02:45:39 |   Checkpoint saved at sample 250
02:59:18 |   [260/350] Accuracy so far: 217/260 = 83.5%
03:12:44 |   [270/350] Accuracy so far: 225/270 = 83.3%
03:26:52 |   [280/350] Accuracy so far: 235/280 = 83.9%
03:41:02 |   [290/350] Accuracy so far: 245/290 = 84.5%
