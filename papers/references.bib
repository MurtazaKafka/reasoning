@inproceedings{rafailov2023direct,
  title={{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

@article{hu2022lora,
  title={{LoRA: Low-Rank Adaptation of Large Language Models}},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2022}
}

@article{wei2022chain,
  title={{Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@article{cobbe2021gsm8k,
  title={{Training Verifiers to Solve Math Word Problems}},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{cobbe2021training,
  title={{Training Verifiers to Solve Math Word Problems}},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{wang2023selfconsistency,
  title={{Self-Consistency Improves Chain of Thought Reasoning in Language Models}},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2023}
}

@article{yao2023tree,
  title={{Tree of Thoughts: Deliberate Problem Solving with Large Language Models}},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@article{lightman2023lets,
  title={{Let's Verify Step by Step}},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{madaan2023self,
  title={{Self-Refine: Iterative Refinement with Self-Feedback}},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{pan2023automatically,
  title={{Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Self-Correction Strategies}},
  author={Pan, Liangming and Saxon, Michael and Xu, Wenda and Nathani, Deepak and Wang, Xinyi and Wang, William Yang},
  journal={arXiv preprint arXiv:2308.03188},
  year={2023}
}

@article{jiang2024fobar,
  title={{FOBAR: Forward-Backward Reasoning in Language Models for Verification}},
  author={Jiang, Weisen and others},
  journal={arXiv preprint},
  year={2024}
}

@article{liu2024reverse,
  title={{Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning}},
  author={Liu, Yinger and others},
  journal={arXiv preprint},
  year={2024}
}

@article{chen2025bidirectional,
  title={{Bidirectional Reasoning for Improved Factual Accuracy}},
  author={Chen, Zhi and others},
  journal={arXiv preprint},
  year={2025}
}

@article{lai2024step,
  title={{Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs}},
  author={Lai, Xin and others},
  journal={arXiv preprint arXiv:2406.18629},
  year={2024}
}

@article{dettmers2023qlora,
  title={{QLoRA: Efficient Finetuning of Quantized LLMs}},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{huang2023survey,
  title={{A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

@article{vaswani2017attention,
  title={{Attention Is All You Need}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017}
}
